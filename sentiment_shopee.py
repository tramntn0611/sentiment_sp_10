import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split  
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix
from sklearn. metrics import classification_report, roc_auc_score, roc_curve
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
import pickle
import regex
import re
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import metrics
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize
from underthesea import word_tokenize, pos_tag, sent_tokenize
from wordcloud import WordCloud
from streamlit_option_menu import option_menu
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

nltk.download('punkt')


#Text Processing
##LOAD EMOJICON
with open('emojicon.txt', 'r', encoding="utf8") as file:  
    emoji_lst = file.read().split('\n')
    emoji_dict = {}
    for line in emoji_lst:
        key, value = line.split('\t')
        emoji_dict[key] = str(value)

#################
#LOAD TEENCODE
with open('teencode.txt', 'r', encoding="utf8") as file:  
    teen_lst = file.read().split('\n')
    teen_dict = {}
    for line in teen_lst:
        key, value = line.split('\t')
        teen_dict[key] = str(value)

###############
#LOAD TRANSLATE ENGLISH -> VNMESE
with open('english-vnmese.txt', 'r', encoding="utf8") as file:  
    english_lst = file.read().split('\n')
    english_dict = {}
    for line in english_lst:
        key, value = line.split('\t')
        english_dict[key] = str(value)

################
#LOAD wrong words
with open('wrong-word.txt', 'r', encoding="utf8") as file:  
    wrong_lst = file.read().split('\n')

#################
#LOAD STOPWORDS
with open('vietnamese-stopwords.txt', 'r', encoding="utf8") as file:  
    stopwords_lst = file.read().split('\n')
  

def process_text_str(text, emoji_dict, teen_dict, wrong_lst):
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(re.findall(pattern,sentence))
        # ...
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document

def process_text(text, emoji_dict, teen_dict, wrong_lst):
    if isinstance(text, float):
        text = str(text)
    document = text.lower()
    document = document.replace("â€™",'')
    document = regex.sub(r'\.+', ".", document)
    new_sentence =''
    for sentence in sent_tokenize(document):
        # if not(sentence.isascii()):
        ###### CONVERT EMOJICON
        sentence = ''.join(emoji_dict[word]+' ' if word in emoji_dict else word for word in list(sentence))
        ###### CONVERT TEENCODE
        sentence = ' '.join(teen_dict[word] if word in teen_dict else word for word in sentence.split())
        ###### DEL Punctuation & Numbers
        pattern = r'(?i)\b[a-zÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]+\b'
        sentence = ' '.join(regex.findall(pattern,sentence))
        # ...
        ###### DEL wrong words
        sentence = ' '.join('' if word in wrong_lst else word for word in sentence.split())
        new_sentence = new_sentence+ sentence + '. '
    document = new_sentence
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    #...
    return document



# Chuáº©n hÃ³a unicode tiáº¿ng viá»‡t
def loaddicchar():
    uniChars = "Ã Ã¡áº£Ã£áº¡Ã¢áº§áº¥áº©áº«áº­Äƒáº±áº¯áº³áºµáº·Ã¨Ã©áº»áº½áº¹Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­á»‰Ä©á»‹Ã²Ã³á»Ãµá»Ã´á»“á»‘á»•á»—á»™Æ¡á»á»›á»Ÿá»¡á»£Ã¹Ãºá»§Å©á»¥Æ°á»«á»©á»­á»¯á»±á»³Ã½á»·á»¹á»µÃ€Ãáº¢Ãƒáº Ã‚áº¦áº¤áº¨áºªáº¬Ä‚áº°áº®áº²áº´áº¶ÃˆÃ‰áººáº¼áº¸ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃá»ˆÄ¨á»ŠÃ’Ã“á»Ã•á»ŒÃ”á»’á»á»”á»–á»˜Æ á»œá»šá»á» á»¢Ã™Ãšá»¦Å¨á»¤Æ¯á»ªá»¨á»¬á»®á»°á»²Ãá»¶á»¸á»´Ã‚Ä‚ÄÃ”Æ Æ¯"
    unsignChars = "aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU"

    dic = {}
    char1252 = 'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'.split(
        '|')
    charutf8 = "Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´".split(
        '|')
    for i in range(len(char1252)):
        dic[char1252[i]] = charutf8[i]
    return dic

# ÄÆ°a toÃ n bá»™ dá»¯ liá»‡u qua hÃ m nÃ y Ä‘á»ƒ chuáº©n hÃ³a láº¡i
def convert_unicode(txt):
    dicchar = loaddicchar()
    return regex.sub(
        r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£',
        lambda x: dicchar[x.group()], txt)



def process_special_word(text):
    # cÃ³ thá»ƒ cÃ³ nhiá»u tá»« Ä‘áº·c biá»‡t cáº§n rÃ¡p láº¡i vá»›i nhau
    new_text = ''
    text_lst = text.split()
    i= 0
    # khÃ´ng, cháº³ng, cháº£...
    if 'khÃ´ng' in text_lst:
        while i <= len(text_lst) - 1:
            word = text_lst[i]
            #print(word)
            #print(i)
            if  word == 'khÃ´ng':
                next_idx = i+1
                if next_idx <= len(text_lst) -1:
                    word = word +'_'+ text_lst[next_idx]
                i= next_idx + 1
            else:
                i = i+1
            new_text = new_text + word + ' '
    else:
        new_text = text
    return new_text.strip()



import re
# HÃ m Ä‘á»ƒ chuáº©n hÃ³a cÃ¡c tá»« cÃ³ kÃ½ tá»± láº·p
def normalize_repeated_characters(text):
    # Thay tháº¿ má»i kÃ½ tá»± láº·p liÃªn tiáº¿p báº±ng má»™t kÃ½ tá»± Ä‘Ã³
    # VÃ­ dá»¥: "ngonnnn" thÃ nh "ngon", "thiá»‡tttt" thÃ nh "thiá»‡t"
    return re.sub(r'(.)\1+', r'\1', text)

# Ãp dá»¥ng hÃ m chuáº©n hÃ³a cho vÄƒn báº£n
# print(normalize_repeated_characters(example))



def process_postag_thesea(text):
    new_document = ''
    for sentence in sent_tokenize(text):
        sentence = sentence.replace('.','')
        ###### POS tag
        lst_word_type = ['N','Np','A','AB','V','VB','VY','R']
        # lst_word_type = ['A','AB','V','VB','VY','R']
        sentence = ' '.join( word[0] if word[1].upper() in lst_word_type else '' for word in pos_tag(process_special_word(word_tokenize(sentence, format="text"))))
        new_document = new_document + sentence + ' '
    ###### DEL excess blank space
    new_document = regex.sub(r'\s+', ' ', new_document).strip()
    return new_document


stop_words = [
    'nhÃ  hÃ ng', 'nhÃ _hÃ ng', 'quÃ¡n', 'quÃ¡n_Äƒn', 'Äƒn', 'Ä‘á»“_Äƒn', 'bá»¯a', 'buá»•i', 'trÆ°a', 'tá»‘i', 'hÃ´m nay', 'ngÃ y mai'
    'sÃ¡ng', 'thá»±c_Ä‘Æ¡n', 'mÃ³n', 'mÃ³n_Äƒn', 'bÃ n', 'Ä‘áº·t bÃ n', 'Ä‘áº·t_bÃ n', 'nhÃ¢n_viÃªn', 'phá»¥c_vá»¥', 'dá»‹ch_vá»¥',
    'khÃ¡ch_hÃ ng', 'khÃ¡ch', 'Ä‘á»“_uá»‘ng', 'mÃ³n', 'mÃ³n_Äƒn' 'giÃ¡', 'hÃ³a_Ä‘Æ¡n',
    'pháº£i_chÄƒng', 'khÃ´ng_gian', 'trang_trÃ­', 'chá»—', 'vá»‹_trÃ­', 'khu_vá»±c', 'cháº¥t_lÆ°á»£ng', 'sá»‘_lÆ°á»£ng',
    'Ä‘á»_nghá»‹', 'gá»£i_Ã½', 'tráº£i_nghiá»‡m', 'thá»­', 'thÆ°á»Ÿng_thá»©c', 'Ä‘Ã¡nh_giÃ¡', 'sao', 'lÃ ', 'luÃ´n',  'cÃ³',
    'ná»¯a', 'nÃ³i', 'tháº¥y', 'quÃ¡', 'cÅ©ng', 'lÃ m', 'cÃ²n', 'bÃ¡nh','ngÆ°á»i', 'thÃªm', 'khÃ¡c', 'báº¡n', 'láº¡i', 'nhÃ¬n',
    'pháº§n', 'gá»i', 'bÃªn', 'chá»‰', 'lÃªn', 'gÃ ', 'bÃ¡n', 'cháº¯c', 'pháº£i',  'lÃºc', 'Ä‘i', 'kiá»ƒu', 'cÆ¡m', 'Ä‘áº·t', 'vá»',
    'chÆ°a', 'kÃªu',  'mÃ¬', 'bÃ¡nh_mÃ¬', 'hÃ´m', 'thá»‹t', 'tÃ´', 'Ä‘áº¿n', 'hÃ ng', 'nÆ°á»›c cháº¥m', 'tÃ­nh', 'nÆ°á»›c dÃ¹ng', 'ngá»“i',
    'nÆ°á»›c lÃ¨o', 'láº§n Ä‘áº§u', 'láº¥y', 'há»™p', 'kÃ¨m', 'cá»©', 'tiá»‡m', 'nhÃ ', 'tá»›i', 'bÃ²', 'sá»‘t', 'Ä‘á»“', 'bá»', 'cÆ¡m gÃ ', 'chá»§',
    'cÅ©ng ráº¥t', 'ngÃ y', 'thÆ°á»ng', 'cÃ²n cÃ³', 'giá»‘ng', 'cháº£', 'sÃ¡ng', 'ghÃ©', 'tháº¥y cÅ©ng', 'thá»©', 'chá»n', 'toÃ n', 'cÃ³ thÃªm',
    'vÃ o', 'riÃªng', 'Ä‘em', 'giÃ¡ cÅ©ng', 'há»i', 'sáº½', 'loáº¡i', 'vÃ´', 'Äƒn_á»Ÿ', 'cÃ¡ch', 'phÃ´', 'thá»©c_Äƒn', 'cháº¡y', 'giá»¯', 'chÃ¡o',
    'phá»Ÿ', 'Ä‘ang', 'bÃºn', 'tÃ´m', 'tháº¥y cÃ³', 'á»‘c', 'thá»‹t bÃ²', 'dÄ©a', 'cho', 'gá»i pháº§n', 'cá»±c', 'cÃ²n láº¡i','lÃºc_nÃ o cÅ©ng',
    'Ä‘i ngÆ°á»i', 'uá»‘ng', 'quáº­n', 'xÃ´i', 'chÃ¨', 'váº«n cÃ²n', 'nhÆ°_váº­y', 'má»Ÿ', 'báº£o', 'cÃ¹ng', 'Ä‘Æ°a', 'vá»‹t', 'ráº¥t lÃ ', 'nÆ°á»›c_máº¯m',
    'lÃ  tháº¥y', 'Ä‘Æ°á»ng', 'pháº§n cÆ¡m', 'gá»­i', 'táº§m', 'máº·t', 'trÆ°á»›c', 'Ä‘Æ°á»£c','láº¯m', 'ráº¥t', 'giÃ¡', 'khÃ¡', 'hÆ¡n', 'váº«n', 'háº¿t', 'láº§n', 'má»›i', 'khÃ´ng_cÃ³',
    'cÃ³_thá»ƒ', 'giá»', 'Ä‘á»u', 'biáº¿t', 'Ä‘Ãºng', 'khÃ´ng_biáº¿t', 'khÃ´ng_pháº£i', 'nÆ°á»›ng', 'hÆ¡i', 'nhiá»u láº§n',
   'láº§n Ä‘áº§u', 'nghÄ©', 'chiÃªn', 'Ä‘á»§', 'nhÃ¡nh', 'ngoÃ i', 'cÃ¡', 'Ä‘iá»ƒm', 'nhÆ°ng_mÃ ', 'hÃ¬nh', 'dá»‹p', 'nÆ¡i', 'chiá»u','trÃªn', 'trá»™n', 'cáº£m_giÃ¡c',
   'liá»n', 'hÃ¬nh_nhÆ°', 'miáº¿ng', 'náº¥u', 'tá»«ng', 'náº±m', 'sáºµn', 'sá»‘', 'máº¥t', 'nhá»›', 'chÃ©n', 'khoáº£ng', 'láº§n láº§n', 'khÃ´ng_tháº¥y',
   'Ä‘á»•i', 'cáº§n', 'máº¹', 'á»•', 'nháº­n', 'gá»“m', 'láº§n Ä‘áº§u_tiÃªn', 'khá»i', 'tÃ­', 'khÃ´ng', 'nhiÃªn', 'máº·c_dÃ¹', 'giÃ²', 'Ã¡', 'Ä‘áº§u',
   'nháº­n Ä‘Æ°á»£c', 'trá»i', 'giáº£m', 'viá»‡c', 'cá»±c_kÃ¬', 'tiáº¿p', 'Ä‘á»£i', 'rÃ¡n', 'lÃºc_nÃ o',  'cÃ³_Ä‘iá»u', 'láº§u', 'sá»£i', 'cháº³ng', 'cuá»‘n', 'thÃ nh', 'xuá»‘ng',
   'review', 'há»“i', 'bá»‹ch', 'miá»‡ng', 'dÃ¹ng', 'Ä‘Ã¹i', 'tÃ¢y', 'khÃ´ng_bá»‹', 'tÃªn', 'cáº£m_nháº­n', 'nhÃ³m',
   'tráº£', 'gá»i', 'hÆ¡n nhiá»u', 'nÃªn', 'má»›i Ä‘Æ°á»£c']




def remove_stopword(text):
    ###### REMOVE stop words
    document = ' '.join('' if word in stop_words else word for word in text.split())
    #print(document)
    ###### DEL excess blank space
    document = regex.sub(r'\s+', ' ', document).strip()
    return document


def clean_text_df(text):
  clean_text = text.apply(lambda x: process_text(x, emoji_dict, teen_dict, wrong_lst))
  clean_text = clean_text.apply(convert_unicode)
  clean_text = clean_text.apply(process_special_word)
  clean_text = clean_text.apply(normalize_repeated_characters)
  clean_text = clean_text.apply(process_postag_thesea)
  clean_text = clean_text.apply(remove_stopword)
  return clean_text

def clean_text_str(text):
  clean_text = process_text_str(text, emoji_dict, teen_dict, wrong_lst)
  clean_text = convert_unicode(clean_text)
  clean_text = process_special_word(clean_text)
  clean_text = normalize_repeated_characters(clean_text)
  clean_text = process_postag_thesea(clean_text)
  clean_text = remove_stopword(clean_text)
  return clean_text

def predict_sentiment(text):
    return 'ğŸ˜Š' if text == 1 else 'ğŸ˜'  

# Upload file
data = pd.read_csv('data_sentiment.csv')
data['Date'] = data['Time'].map(lambda x: x.split()[0])
data['Hour'] = data['Time'].map(lambda x: x.split()[1].split(':')[0])
data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')
data['Hour'] = data['Hour'].astype(int)
data['Month'] = data['Date'].dt.month
data['Year'] = data['Date'].dt.year

#load model
with open('pipeline.pkl', 'rb') as file:  
    sent_model = pickle.load(file)


#GUI
st.set_page_config(page_title='Sentiment Analysis', page_icon='ğŸ“Š', layout="centered")

menu = ["Giá»›i thiá»‡u", "PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡", "ThÃ´ng tin nhÃ  hÃ ng", "Xem thÃªm"]
choice = option_menu (
                    menu_title=None,
                    options = menu,
                    menu_icon = 'cast',
                    default_index = 0,
                    orientation = 'horizontal')

if choice == 'Giá»›i thiá»‡u': 
    st.title("Project 4")
    st.subheader("Nguyá»…n Thá»‹ Ngá»c TrÃ¢m - ÄÃ o Minh TrÃ­")
    st.subheader("ğŸ™‚ğŸ˜â˜¹ï¸ Sentiment Analysis")
    st.write("""
    - Sentiment Analysis lÃ  quÃ¡ trÃ¬nh phÃ¢n tÃ­ch, Ä‘Ã¡nh giÃ¡ quan Ä‘iá»ƒm cá»§a má»™t ngÆ°á»i vá» má»™t Ä‘á»‘i tÆ°á»£ng nÃ o Ä‘Ã³ (quan Ä‘iá»ƒm mang tÃ­nh tÃ­ch cá»±c, tiÃªu cá»±c, hay trung tÃ­nh,..). QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ thá»±c hiá»‡n báº±ng viá»‡c sá»­ dá»¥ng cÃ¡c táº­p luáº­t (rule-based), sá»­ dá»¥ng  Machine Learning hoáº·c phÆ°Æ¡ng phÃ¡p Hybrid (káº¿t há»£p hai  phÆ°Æ¡ng phÃ¡p trÃªn).
    - Sentiment Analysis Ä‘Æ°á»£c á»©ng dá»¥ng nhiá»u trong thá»±c táº¿, Ä‘áº·c biá»‡t lÃ  trong hoáº¡t Ä‘á»™ng quáº£ng bÃ¡ kinh doanh. Viá»‡c phÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ cá»§a ngÆ°á»i dÃ¹ng vá» má»™t sáº£n pháº©m xem há» Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c, tÃ­ch cá»±c hoáº·c Ä‘Ã¡nh giÃ¡ cÃ¡c háº¡n cháº¿ cá»§a sáº£n pháº©m sáº½ giÃºp cÃ´ng ty nÃ¢ng cao cháº¥t lÆ°á»£ng sáº£n pháº©m vÃ  tÄƒng cÆ°á»ng hÃ¬nh áº£nh cá»§a cÃ´ng ty, cá»§ngcá»‘ sá»± hÃ i lÃ²ng cá»§a khÃ¡ch hÃ ng.""")  
    st.image('sentimentanalysishotelgeneric-2048x803-1.jpg')
    st.subheader("ğŸ½ï¸ Sentiment Analysis trong áº©m thá»±c")
    st.write("""
    - Äá»ƒ lá»±a chá»n má»™t nhÃ  hÃ ng/quÃ¡n Äƒn má»›i chÃºng ta cÃ³ xu hÆ°á»›ng xem xÃ©t nhá»¯ng bÃ¬nh luáº­n tá»« nhá»¯ng ngÆ°á»i Ä‘Ã£ thÆ°á»Ÿng thá»©c Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh cÃ³ nÃªn thá»­ hay khÃ´ng?
    - XÃ¢y dá»±ng há»‡ thá»‘ng há»— trá»£ nhÃ  hÃ ng/quÃ¡n Äƒn phÃ¢n loáº¡i cÃ¡c pháº£n há»“i cá»§a khÃ¡ch hÃ ng  thÃ nh cÃ¡c nhÃ³m: tÃ­ch cá»±c, tiÃªu cá»±c, trung tÃ­nh  dá»±a trÃªn dá»¯ liá»‡u dáº¡ng vÄƒn báº£n.
    - Tá»« nhá»¯ng Ä‘Ã¡nh giÃ¡ cá»§a khÃ¡ch hÃ ng, váº¥n Ä‘á» Ä‘Æ°á»£c Ä‘Æ°a ra lÃ  lÃ m sao Ä‘á»ƒ cÃ¡c nhÃ  hÃ ng/ quÃ¡n Äƒn hiá»ƒu Ä‘Æ°á»£c khÃ¡ch hÃ ng rÃµ hÆ¡n, biáº¿t há» Ä‘Ã¡nh giÃ¡ vá» mÃ¬nh nhÆ° tháº¿ nÃ o Ä‘á»ƒ cáº£i thiá»‡n hÆ¡n trong dá»‹ch vá»¥/ sáº£n pháº©m.""")
    st.image('vn-11134513-7r98o-lugftthr8is27b.png')
    st.subheader("ğŸ‘©â€ğŸ’» CÃ¡c bÆ°á»›c thá»±c hiá»‡n")
    st.image('project-10.jpg')

elif choice == 'Xem thÃªm':
    data_review = pd.read_csv('data_review_merge.csv')
    restaurant = pd.read_csv('1_Restaurants.csv')
    review = pd.read_csv('2_Reviews.csv')
    st.title("ğŸ” Data Review ğŸ“")
    st.write("""
    - Dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p sáºµn trong táº­p tin 2_Reviews.csv vá»›i gáº§n 30.000 máº«u gá»“m cÃ¡c thÃ´ng tin:
    - ID (mÃ£), User (ngÆ°á»i dÃ¹ng), Time (thá»i gian Ä‘Ã¡nh giÃ¡), Rating (Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡), Comment (ná»™i dung Ä‘Ã¡nh giÃ¡), vÃ  IDRestaurant (mÃ£ nhÃ  hÃ ng)
    - Táº­p tin chá»©a thÃ´ng tin vá» nhÃ  hÃ ng: 1_Restaurants.csv vá»›i hÆ¡n 1.600 máº«u gá»“m cÃ¡c thÃ´ng tin:
    - 'ID (mÃ£), Restaurant (tÃªn nhÃ  hÃ ng), Address (Ä‘á»‹a chá»‰), Time (giá» má»Ÿ cá»­a), Price (khoáº£ng giÃ¡), District(quáº­n)""")
    st.markdown(
                f"""
                <div style="text-align: center;">
                    <h3 style="margin-bottom: 0;">1_Restaurants</h3>
                </div>
                """, 
                unsafe_allow_html=True
            )
    st.dataframe(restaurant)
    st.markdown(
                f"""
                <div style="text-align: center;">
                    <h3 style="margin-bottom: 0;">2_Reviews</h3>
                </div>
                """, 
                unsafe_allow_html=True
            )
    st.dataframe(review)
    st.markdown(f"**Tá»•ng sá»‘ lÆ°á»£ng nhÃ  hÃ ng: {len(list(restaurant['Restaurant'].value_counts().index))}**")

    dis_res = restaurant.groupby('District')['Restaurant'].count().sort_values(ascending=False)
    top_10 = data_review.groupby('Restaurant')['Rating'].count().sort_values(ascending=False).head(10)
  
    st.markdown("**Tá»•ng sá»‘ lÆ°á»£ng nhÃ  hÃ ng theo quáº­n**")
    plt.figure(figsize=(10, 8))
    ax = dis_res.plot(kind='barh', x='District', y='Number of Restaurants', legend=False)
    ax.set_xlabel("Number of Restaurants")
    plt.title('Number of Restaurants by District')
    for container in ax.containers:
        ax.bar_label(container, label_type='edge')
    st.pyplot(plt)    
   
    st.markdown("**Top 10 nhÃ  hÃ ng cÃ³ sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡ nhiá»u nháº¥t**")
    plt.figure(figsize=(10, 8))
    axtop10 = top_10.plot(kind='barh')
    axtop10.set_xlabel("Number of Reviews")
    plt.title('Top 10 Restaurants with Most Reviews')
    for container in axtop10.containers:
        axtop10.bar_label(container, label_type='edge')
    st.pyplot(plt)    
 
    df_plot_sent = data['Rating_Score'].value_counts()
    st.markdown("**PhÃ¢n phá»‘i Rating**")
    fig, ax = plt.subplots()
    ax.pie(df_plot_sent, labels=df_plot_sent.index, autopct='%1.1f%%', startangle=90,colors=['#66b3ff', '#ff9999', '#99ff99'])
    ax.axis('equal') 
    st.pyplot(fig)

    st.title("ğŸ“ˆ Huáº¥n luyá»‡n mÃ´ hÃ¬nh")
    st.subheader("CÃ¡c bÆ°á»›c thá»±c hiá»‡n")
    st.image('project-10.jpg')
    st.write("""
    - MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i cáº£ 3 táº­p train (OverSampling, UnderSampling vÃ  Táº­p train gá»‘c)
    - Káº¿t quáº£ huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u gá»‘c khÃ´ng tá»‘t do dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng khÃ¡ náº·ng
    - Qua 2 phÆ°Æ¡ng phÃ¡p cáº£i thiá»‡n káº¿t quáº£ vá»›i viá»‡c chia táº­p train qua phÆ°Æ¡ng phÃ¡p OverSampling vÃ  UnderSampling
    - PhÆ°Æ¡ng phÃ¡p UnderSampling táº­p train vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘em láº¡i káº¿t quáº£ khÃ¡ tá»‘t
    - Sau Ä‘Ã¢y lÃ  káº¿t quáº£ cá»§a cÃ¡c mÃ´ hÃ¬nh khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p train Ä‘Æ°á»£c UnderSampling""")

    left_col6, right_col6 = st.columns(2)
    with left_col6:
        st.subheader('Logistic Regression')
        st.image('LR.JPG')
    with right_col6:
        st.subheader('NaiveBayes')
        st.image('NB.JPG')

    left_col7, right_col7 = st.columns(2)
    with left_col7:
        st.subheader('KNeighbors_3Classifier')
        st.image('KNN.JPG')
    with right_col7:
        st.subheader('KNeighbors_5Classifier')
        st.image('KNN5.JPG')

    left_col8, right_col8 = st.columns(2)
    with left_col8:
        st.subheader('DecisionTreeClassifier')
        st.image('DT.JPG')
    with right_col8:
        st.subheader('RandomForestClassifier')
        st.image('RF.JPG')

    left_col9, right_col9 = st.columns(2)
    with left_col9:
        st.subheader('SVC_Rbf')
        st.image('SVC_RBF.JPG')
    with right_col9:
        st.subheader('SVC_Linear')
        st.image('SVC_LINEAR.JPG')

    left_col10, right_col10 = st.columns(2)
    with left_col10:
        st.subheader('AdaBoostClassifier')
        st.image('ADA.JPG')
    with right_col10:
        st.subheader('XGBClassifier')
        st.image('XGB.JPG')


    st.write("**Nháº­n xÃ©t vÃ  lá»±a chá»n mÃ´ hÃ¬nh**")
    st.write("""
    - Do dá»¯ liá»‡u bá»‹ máº¥t cÃ¢n báº±ng, Precision Score cÃ³ thá»ƒ sáº½ tháº¥p, ta cÃ³ thá»ƒ chá»‰ xem Recall Score vÃ  Accuracy Score Ä‘á»ƒ Ä‘Ã¡nh giÃ¡
    - MÃ´ hÃ¬nh SVC vá»›i kernel='rbf' vá»›i táº­p train Ä‘Æ°á»£c UnderSampling cho káº¿t quáº£ tá»‘t nháº¥t
    - Chá»‰ sá»‘ Recall cho 2 nhÃ£n Ä‘á»u khÃ¡ cao (0.8 ~ 0.9), Accuracy Score Ä‘áº¡t 0.88
    - HÆ¡n ná»¯a so sÃ¡nh trá»±c quan Cofusion Matrix cho tháº¥y nhÃ£n Positive vÃ  Negative dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tá»‘t nháº¥t trong táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh trÃªn
    - Do Ä‘Ã³ ta sáº½ chá»n MÃ´ hÃ¬nh SVC vá»›i kernel='rbf' RandomUnderSampling Ä‘á»ƒ dá»± Ä‘oÃ¡n trÃªn toÃ n bá»™ dá»¯ liá»‡u""")




elif choice == 'PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡':
    st.subheader("ğŸ™‚ğŸ˜â˜¹ï¸ PhÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡")
    st.write("""
    - Nháº­p 1 dÃ²ng Ä‘Ã¡nh giÃ¡ hoáº·c táº£i lÃªn 1 báº£ng dá»¯ liá»‡u csv cÃ¡c Ä‘Ã¡nh giÃ¡
    - áº®n nÃºt phÃ¢n tÃ­ch, sáº½ tráº£ ra káº¿t quáº£ nháº­n xÃ©t Ä‘Ã¡nh giÃ¡ Ä‘Ã³ lÃ  Positive (TÃ­ch cá»±c) hay Negative (TiÃªu cá»±c)""")
    st.markdown("##")
    st.subheader("ÄÃ¡nh giÃ¡")
    with st.form(key='TextForm'):
        text = st.text_area("Nháº­p 1 cÃ¢u Ä‘Ã¡nh giÃ¡ vÃ o Ä‘Ã¢y")
        submit_button = st.form_submit_button(label = 'PhÃ¢n tÃ­ch')
    col1, col2 = st.columns(2)
    if submit_button:
        with col1:
            x_new = clean_text_str(text) 
            if isinstance(x_new, str):
                x_new = [x_new] 
            y_pred_new = sent_model.predict(x_new)       
            st.write(y_pred_new)
            if y_pred_new == 1:
                st.markdown("Positive ğŸ™‚")
            else:
                st.markdown("Negative â˜¹ï¸")
        with col2:
            if y_pred_new == 1:
                st.image("smile.png")
            else:
                st.image("sad.png")

    st.subheader('Táº£i tá»‡p')
    with st.form(key='dfform'):
        # Upload file
        uploaded_file = st.file_uploader("Táº£i tá»‡p", type=['csv'])
        submit_button = st.form_submit_button(label = 'PhÃ¢n tÃ­ch')
        if uploaded_file is not None:
            st.markdown('---')
            df = pd.read_csv(uploaded_file, header=None,)
            st.markdown('ÄÃ¡nh giÃ¡ cá»§a ngÆ°á»i dÃ¹ng')
            st.dataframe(df)
            lines = df.iloc[:, 0]    
            if len(lines)>0:
                cleaned_lines = [clean_text_str(str(line)) for line in lines]             
                y_pred_new = sent_model.predict(cleaned_lines)
                df['Sentiment'] = y_pred_new
                df['Tráº¡ng thÃ¡i'] = [predict_sentiment(text) for text in y_pred_new]
                st.markdown('Tráº¡ng thÃ¡i Ä‘Ã¡nh giÃ¡')
                st.dataframe(df)       


elif  choice == 'ThÃ´ng tin nhÃ  hÃ ng':
    data_res = pd.read_csv('df_restaurants_fn.csv')
    res = st.multiselect(
                        "Lá»±a chá»n nhÃ  hÃ ng:", 
                        options = data_res["Restaurant"].unique(),
                        max_selections = 1)
    if res:
        df_selection = data_res.query("Restaurant == @res")
        df_selection2 = data.query("Restaurant == @res")

        if df_selection.empty:
            st.warning("Chá»n 1 nhÃ  hÃ ng tá»« há»™p chá»n trÃªn")
        else:
            st.title(":bar_chart: ThÃ´ng tin nhÃ  hÃ ng")
            st.markdown("##")

            name = df_selection["Restaurant"].values[0]
            rating_score = df_selection["Sentiment"].values[0]
            star_rating = round(df_selection["Rating"].values[0], 1)
            rating = 'â­ï¸' * int(round(df_selection["Rating"].values[0], 0))
            price = df_selection["Price"].values[0]
            time = df_selection["Open_Time"].values[0]
            pos = df_selection["Positive"].values[0]
            neg = df_selection["Negative"].values[0]
            neu = df_selection["Neutral"].values[0]
            dis = df_selection["District"].values[0]
            add = df_selection["Address"].values[0]
            max_hour = df_selection["Most_Reviewed_Hour"].values[0]
            min_hour = df_selection["Min_Reviewed_Hour"].values[0]
            total_rat = int(pos + neg + neu)

            st.markdown(
                f"""
                <div style="text-align: center;">
                    <h1 style="margin-bottom: 0;">{name}</h1>
                </div>
                """, 
                unsafe_allow_html=True
            )

            leftcol1, rightcol1 = st.columns(2)
            with leftcol1:  
                st.markdown(
                f"""
                <div style="text-align: center;">
                    <h1>{star_rating}</h1>
                    <h4>{rating}</h4>
                    <p><strong>{total_rat} Ä‘Ã¡nh giÃ¡</strong></p>
                </div>
                """, 
                unsafe_allow_html=True
            )
            with rightcol1:
                df_plot_rat = df_selection.groupby(['Restaurant']).sum()[['9-10', '7-8', '5-6', '3-4', '1-2']]
                for restaurant, row in df_plot_rat.iterrows():
                    plt.figure(figsize=(12, 6))  # Adjust figure size as needed
                    plt.barh(row.index, row.values, color=['#F9C70C'])
                    plt.xticks(np.arange(len(row)), [])  # Hide x-axis ticks
                    plt.gca().invert_yaxis()  # Invert y-axis to have the highest bar at the top
                    plt.box(False)
                    plt.gca().set_facecolor('none')
                    st.pyplot(plt)
            
            st.markdown(
                f"""
                <div style="text-align: center;">
                    <h3>ğŸ“ Äá»‹a chá»‰</h3>
                    <h3>{add}</h3>
                </div>
                """, 
                unsafe_allow_html=True)

            leftcol2, rightcol2 = st.columns([1.2, 1])
            with leftcol2:  
                st.markdown(
                    f"""
                        <h4>ğŸ•’Giá» hoáº¡t Ä‘á»™ng: {time}</h4>
                    """, 
                    unsafe_allow_html=True
                )
            with rightcol2:  
                st.markdown(
                    f"""
                        <h4>ğŸ·ï¸GiÃ¡: {price}</h4>
                    """,
                    unsafe_allow_html=True
                )
                
            leftcol3, rightcol3 = st.columns([1.2, 1])
            with leftcol2: 
                st.markdown(
                    f"""
                        <h4>ğŸ”¼ÄÃ¡nh giÃ¡ nhiá»u nháº¥t: {max_hour} giá»</h4>
                    """, 
                    unsafe_allow_html=True
                )
            with rightcol2:
                st.markdown(
                    f"""
                        <h4>ğŸ”½ÄÃ¡nh giÃ¡ Ã­t nháº¥t: {min_hour} giá»</h4>
                    """, 
                    unsafe_allow_html=True
                )
            st.markdown("""---""")

            left_col4, right_col4 = st.columns(2)
            with left_col4:
                st.subheader(f"â˜¹ï¸Negative: {neg} Ä‘Ã¡nh giÃ¡")
                st.subheader(f"ğŸ˜ŠPositive: {pos} Ä‘Ã¡nh giÃ¡")
                st.subheader(f"ğŸ˜Neutral: {neu} Ä‘Ã¡nh giÃ¡")
            with right_col4:
                df_plot_sent = df_selection.groupby(['Restaurant']).sum()[['Positive', 'Negative', 'Neutral']]
                for restaurant, row in df_plot_sent.iterrows():
                    plt.figure(figsize=(2, 2))
                    plt.pie(row, labels=row.index, autopct='%1.1f%%', startangle=90, wedgeprops={'edgecolor': 'none'}, colors=['#66b3ff', '#ff9999', '#99ff99'], textprops={'fontsize': 8})
                    plt.gca().set_facecolor('none')
                    st.pyplot(plt)
            st.markdown("""---""")

            left_col5, right_col5 = st.columns(2)
            with left_col5:
                st.subheader("ÄÃ¡nh giÃ¡ Positive")
                pos_text = df_selection["comment_positive"].values[0]
                pw = WordCloud(width=400, height=200, background_color='white').generate(pos_text)
                plt.figure(figsize=(10, 5))
                plt.imshow(pw, interpolation='bilinear')
                plt.axis('off')
                st.pyplot(plt)
            with right_col5:
                st.subheader("ÄÃ¡nh giÃ¡ Negative")
                neg_text = df_selection["comment_negative"].values[0]
                nw = WordCloud(width=400, height=200, background_color='white').generate(neg_text)
                plt.figure(figsize=(10, 5))
                plt.imshow(nw, interpolation='bilinear')
                plt.axis('off')
                st.pyplot(plt)

            left_col6, right_col6 = st.columns(2)
            with left_col6:
                df_pos_cm = df_selection2[(df_selection2['Sentiment'] == 'Positive')]
                df_pos_cm = df_pos_cm[['User', 'Time', 'Comment']]
                st.dataframe(df_pos_cm)
            with right_col6:
                df_neg_cm = df_selection2[(df_selection2['Sentiment'] == 'Negative')]
                df_neg_cm = df_neg_cm[['User', 'Time', 'Comment']]
                st.dataframe(df_neg_cm)


            # Plotting function
            def plot_sentiment_rating_trend(df_selection2, data):
                df_sub = data[data['Restaurant'] == df_selection2["Restaurant"].values[0]]
                grb = df_sub.groupby(['Year', 'Month']).agg({
                    'Sentiment': [('Positive', lambda x: (x == 'Positive').sum()),
                                ('Negative', lambda x: (x == 'Negative').sum())],
                    'Comment': 'count',
                    'Rating': 'mean'
                }).reset_index()
                grb.columns = ['Year', 'Month', 'Positive', 'Negative', 'num_comment', 'Rating']
                grb = grb.sort_values(by=['Year', 'Month'])
                grb['DateTime'] = pd.to_datetime(grb['Month'].astype(str) + '/' + grb['Year'].astype(str), format='%m/%Y')

                plt.figure(figsize=(10, 6))
                plt.plot(grb['DateTime'], grb['Rating'], marker='o', markersize=3, color='green')
                plt.title('Biá»ƒu Ä‘á»“ Rating trung bÃ¬nh theo thá»i gian')
                plt.xlabel('Thá»i gian')
                plt.ylabel('Rating')
                plt.ylim(0, 11)
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(plt)

                plt.figure(figsize=(10, 6))
                plt.plot(grb['DateTime'], grb['Positive'], label='Positive', color='green', marker='o', markersize=3)
                plt.plot(grb['DateTime'], grb['Negative'], label='Negative', color='red', marker='o', markersize=3)
                plt.title('Biá»ƒu Ä‘á»“ cÃ¡c loáº¡i Sentiment theo thá»i gian')
                plt.xlabel('Thá»i gian')
                plt.ylabel('Sá»‘ lÆ°á»£ng')
                plt.legend()
                plt.xticks(rotation=45)
                plt.tight_layout()
                st.pyplot(plt)

            plot_sentiment_rating_trend(df_selection2, data)
    else:
        st.warning("Chá»n 1 nhÃ  hÃ ng tá»« há»™p chá»n trÃªn")


                




        


           
